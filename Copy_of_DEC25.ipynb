{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishita300/rishita/blob/master/Copy_of_DEC25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "FDLdR59YQdav"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(600)\n",
        "\n",
        "\n",
        "\n",
        "class DriveFile:\n",
        "    def __init__(self, base_path=\"\"):\n",
        "        self.path = base_path\n",
        "\n",
        "    def dataload(self, folder_path):\n",
        "        img = []\n",
        "        label = []\n",
        "        self.path1 = os.path.join(self.path, folder_path)\n",
        "\n",
        "        if os.path.exists(self.path1):\n",
        "            for i in os.listdir(self.path1):\n",
        "                self.dis_path = os.path.join(self.path1, i)\n",
        "\n",
        "                # Skip invalid directories\n",
        "                if not os.path.exists(self.dis_path) or '.DS_Store' in self.dis_path:\n",
        "                    print(f\"Skipping invalid path or .DS_Store: {self.dis_path}\")\n",
        "                    continue\n",
        "\n",
        "                for j in os.listdir(self.dis_path):\n",
        "                    if j.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                        self.img_path = os.path.join(self.dis_path, j)\n",
        "                        loaded_img = self.imgload(self.img_path)\n",
        "                        if loaded_img is not None:\n",
        "                            img.append(loaded_img)\n",
        "                            label.append(i)\n",
        "                        else:\n",
        "                            print(f\"Skipping non-image file or invalid image: {j}\")\n",
        "            if img and label:\n",
        "                return np.array(img), np.array(label)\n",
        "            else:\n",
        "                print(\"No valid images found in the specified path.\")\n",
        "                return None, None\n",
        "        else:\n",
        "            print(\"Base path not found:\", self.path1)\n",
        "            return \"patherror\", \"patherror\"\n",
        "\n",
        "    def imgload(self, imgpath):\n",
        "        try:\n",
        "            img = cv2.imread(imgpath)\n",
        "            if img is None:\n",
        "                print(f\"Failed to load image at {imgpath}\")\n",
        "                return None\n",
        "            img = cv2.resize(img, (20, 20))  # Downsample to 20x20 (or change size if needed)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "            img = img.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "            return np.array(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {imgpath}: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7SUApqo7g0U",
        "outputId": "dd6028fb-6996-4e5a-afd4-5c9a6493778d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive = DriveFile()\n",
        "\n",
        "# Load training data\n",
        "images, labels = drive.dataload('/content/drive/MyDrive/train_set')\n",
        "if images is not None and labels is not None:\n",
        "    images = (images - np.mean(images)) / np.std(images)  # Normalize training images\n",
        "else:\n",
        "    print(\"Failed to load training data.\")\n",
        "\n",
        "# Load testing data\n",
        "test_images, test_labels = drive.dataload('/content/drive/MyDrive/test_set')\n",
        "if test_images is not None and test_labels is not None:\n",
        "    test_images = (test_images - np.mean(test_images)) / np.std(test_images)  # Normalize testing images\n",
        "else:\n",
        "    print(\"Failed to load testing data.\")"
      ],
      "metadata": {
        "id": "yDIyBKeRQwag"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_labels(labels, label_to_index):\n",
        "    return np.array([label_to_index[label] for label in labels])\n",
        "\n",
        "def one_hot_encode(labels, unique_labels):\n",
        "    return np.eye(len(unique_labels))[labels]\n",
        "\n",
        "# Flattening images\n",
        "images = np.array([img.flatten() for img in images])\n",
        "test_images = np.array([img.flatten() for img in test_images])\n",
        "\n",
        "# Label encoding\n",
        "labels = np.array(labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Generate label-to-index mapping\n",
        "unique_labels = np.unique(labels)\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Integer encoding\n",
        "integer_encoded = encode_labels(labels, label_to_index)\n",
        "test_integer_encoded = encode_labels(test_labels, label_to_index)\n",
        "\n",
        "# One-hot encoding\n",
        "one_hot_encoded = one_hot_encode(integer_encoded, unique_labels)\n",
        "test_one_hot_encoded = one_hot_encode(test_integer_encoded, unique_labels)\n",
        "\n",
        "# Assign one-hot encoded labels back\n",
        "labels = one_hot_encoded\n",
        "test_labels = test_one_hot_encoded"
      ],
      "metadata": {
        "id": "YVOB8QrDQ_5P"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Convert one-hot encoded labels back to integer labels\n",
        "labels_int = np.argmax(labels, axis=1)\n",
        "test_labels_int = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Standardize the images\n",
        "scaler = StandardScaler()\n",
        "images_scaled = scaler.fit_transform(images)\n",
        "test_images_scaled = scaler.transform(test_images)\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "parameters = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "grid_search = GridSearchCV(SVC(random_state=42), parameters, cv=5)\n",
        "grid_search.fit(images_scaled, labels_int)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Initialize and train SVM model with best parameters\n",
        "svm_model = grid_search.best_estimator_\n",
        "svm_model.fit(images_scaled, labels_int)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "svm_predictions = svm_model.predict(test_images_scaled)\n",
        "svm_accuracy = accuracy_score(test_labels_int, svm_predictions)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(f\"SVM Accuracy: {svm_accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels_int, svm_predictions))\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(test_labels_int, svm_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TTPpMGlSO_b",
        "outputId": "402596b6-f97f-471b-8187-9307e7f6c922"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'kernel': 'rbf'}\n",
            "SVM Accuracy: 77.68%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.67      0.66        33\n",
            "           1       0.70      0.80      0.74        20\n",
            "           2       0.92      0.75      0.83        32\n",
            "           3       0.61      0.67      0.64        33\n",
            "           4       0.86      0.78      0.82        23\n",
            "           5       0.70      0.76      0.73        25\n",
            "           6       0.97      0.97      0.97        34\n",
            "           7       0.84      0.82      0.83        33\n",
            "\n",
            "    accuracy                           0.78       233\n",
            "   macro avg       0.78      0.78      0.78       233\n",
            "weighted avg       0.79      0.78      0.78       233\n",
            "\n",
            "Confusion Matrix:\n",
            "[[22  0  1  5  2  2  0  1]\n",
            " [ 0 16  0  3  0  1  0  0]\n",
            " [ 1  3 24  2  0  1  0  1]\n",
            " [ 6  1  1 22  0  1  1  1]\n",
            " [ 1  1  0  0 18  2  0  1]\n",
            " [ 3  0  0  2  0 19  0  1]\n",
            " [ 0  1  0  0  0  0 33  0]\n",
            " [ 1  1  0  2  1  1  0 27]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input=None\n",
        "        self.output=None\n",
        "    def forward(self,input):\n",
        "        # to be overridden\n",
        "        pass\n",
        "    def backward(self,output_gradient,learning_rate):\n",
        "        pass"
      ],
      "metadata": {
        "id": "9ONaOE7QSspJ"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
        "        self.bias = np.zeros((1, output_size))\n",
        "        self.velocities_W = np.zeros_like(self.weights)\n",
        "        self.velocities_B = np.zeros_like(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(self.input, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate, momentcoeff):\n",
        "        # Compute gradients\n",
        "        w_gradient = np.dot(self.input.T, output_gradient)  # Gradient w.r.t weights\n",
        "        b_gradient = np.sum(output_gradient, axis=0, keepdims=True)  # Gradient w.r.t bias\n",
        "        input_gradient = np.dot(output_gradient, self.weights.T)  # Gradient w.r.t input\n",
        "\n",
        "        # Apply momentum\n",
        "        self.velocities_W = momentcoeff * self.velocities_W + (1 - momentcoeff) * w_gradient\n",
        "        self.velocities_B = momentcoeff * self.velocities_B + (1 - momentcoeff) * b_gradient\n",
        "\n",
        "        # Update weights and biases with momentum\n",
        "        self.weights -= learning_rate * self.velocities_W\n",
        "        self.bias -= learning_rate * self.velocities_B\n",
        "\n",
        "        # Optionally apply gradient clipping\n",
        "        # max_norm = 5.0\n",
        "        # if np.linalg.norm(w_gradient) > max_norm:\n",
        "        #     w_gradient = w_gradient * (max_norm / np.linalg.norm(w_gradient))\n",
        "\n",
        "        return input_gradient"
      ],
      "metadata": {
        "id": "zaCL1tpETPrv"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_derivative):\n",
        "        # 'activation' is a function pointer to the activation method (e.g., sigmoid, relu)\n",
        "        self.activation = activation\n",
        "        self.activation_derivative = activation_derivative\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate, momentcoeff):\n",
        "        # Compute the gradient of the loss with respect to the input\n",
        "        activation_gradient = self.activation_derivative(self.input)\n",
        "        # Return the product of the output gradient and the activation gradient\n",
        "        return output_gradient * activation_gradient"
      ],
      "metadata": {
        "id": "nFA9UpNgWGcz"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        # Call the parent constructor with the sigmoid activation function and its derivative\n",
        "        super().__init__(activation=sigmoid, activation_derivative=sigmoid_derivative)"
      ],
      "metadata": {
        "id": "3GEhIMMjWU2P"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    t = tanh(x)\n",
        "    return 1 - (t ** 2)\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        # Call the parent constructor with the tanh activation function and its derivative\n",
        "        super().__init__(activation=tanh, activation_derivative=tanh_derivative)"
      ],
      "metadata": {
        "id": "dVj5RIXNWr_s"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prelu(x, alpha=0.1):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def prelu_derivative(x, alpha=0.1):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "class Relu(Activation):\n",
        "    def __init__(self, alpha=0.1):\n",
        "        # Store alpha for use in PReLU\n",
        "        self.alpha = alpha\n",
        "        # Pass the prelu and prelu_derivative functions to the parent class\n",
        "        super().__init__(activation=lambda x: prelu(x, self.alpha), activation_derivative=lambda x: prelu_derivative(x, self.alpha))"
      ],
      "metadata": {
        "id": "tOJXX5dJW5uS"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
        "        self.bias = np.zeros((1, output_size))\n",
        "        self.velocities_W = np.zeros_like(self.weights)\n",
        "        self.velocities_B = np.zeros_like(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Save input for backward pass\n",
        "        self.input = input\n",
        "        # Compute logits (raw scores)\n",
        "        logits = np.dot(self.input, self.weights) + self.bias\n",
        "        # Numerically stable softmax computation\n",
        "        logits_shifted = logits - np.max(logits, axis=1, keepdims=True)  # For numerical stability\n",
        "        exp_values = np.exp(logits_shifted)\n",
        "        # Softmax output (probabilities)\n",
        "        self.softmax = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        return self.softmax\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate, momentcoeff):\n",
        "        # Gradient of softmax combined with cross-entropy loss\n",
        "        # This simplifies to the difference between the predicted and actual labels\n",
        "        batch_size = self.input.shape[0]\n",
        "\n",
        "        # Compute the gradients for weights, biases, and inputs\n",
        "        # output_gradient is typically the gradient of the loss with respect to the output of this layer\n",
        "        w_gradient = np.dot(self.input.T, output_gradient) / batch_size\n",
        "        b_gradient = np.sum(output_gradient, axis=0, keepdims=True) / batch_size\n",
        "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
        "\n",
        "        # Update weights and biases with momentum\n",
        "        self.velocities_W = momentcoeff * self.velocities_W + (1 - momentcoeff) * w_gradient\n",
        "        self.velocities_B = momentcoeff * self.velocities_B + (1 - momentcoeff) * b_gradient\n",
        "        self.weights -= learning_rate * self.velocities_W\n",
        "        self.bias -= learning_rate * self.velocities_B\n",
        "\n",
        "        return input_gradient"
      ],
      "metadata": {
        "id": "WaCUA8GkXJSe"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(Layer):\n",
        "    def __init__(self, loss_fn, loss_fn_derivative):\n",
        "        # Loss function and its derivative are passed in the constructor\n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_fn_derivative = loss_fn_derivative\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Store predictions and true values for backward pass\n",
        "        self.y_pred = y_pred\n",
        "        self.y_true = y_true\n",
        "        # Return the computed loss value\n",
        "        return self.loss_fn(y_pred, y_true)\n",
        "\n",
        "    def backward(self):\n",
        "        # Return the gradient of the loss with respect to the predictions\n",
        "        return self.loss_fn_derivative(self.y_pred, self.y_true)"
      ],
      "metadata": {
        "id": "abMviiqHXbZm"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryCrossEntropy(Loss):\n",
        "    def __init__(self):\n",
        "        # Binary Cross-Entropy loss function\n",
        "        def bce(y_pred, y_true):\n",
        "            # Clipping the values to avoid log(0) which can cause NaNs\n",
        "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "            return -np.mean((y_true * np.log(y_pred)) + ((1 - y_true) * np.log(1 - y_pred)))\n",
        "\n",
        "        # Derivative of the Binary Cross-Entropy loss function\n",
        "        def bce_derivative(y_pred, y_true):\n",
        "            # Clipping for stability in derivative computation\n",
        "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "            return (y_pred - y_true) / (y_pred * (1 - y_pred))  # Correct derivative expression\n",
        "\n",
        "        super().__init__(bce, bce_derivative)"
      ],
      "metadata": {
        "id": "q-84FqPMXnFY"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseCategoricalCE(Loss):\n",
        "    def __init__(self):\n",
        "        def sce(logits, true_labels):\n",
        "            # Numerically stable softmax computation\n",
        "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Shift logits for numerical stability\n",
        "            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Softmax\n",
        "            true_class_probs = probabilities[np.arange(len(true_labels)), true_labels]  # Select true class probabilities\n",
        "            log_loss = -np.log(true_class_probs + 1e-15)  # Compute log loss, adding a small constant for stability\n",
        "            return np.mean(log_loss)  # Mean log loss over the batch\n",
        "\n",
        "        def sce_derivative(logits, true_labels):\n",
        "            # Softmax derivative (gradient of cross-entropy)\n",
        "            exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Stability trick\n",
        "            probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Softmax probabilities\n",
        "            batch_size = logits.shape[0]\n",
        "\n",
        "            # Subtract 1 from the probability of the true class\n",
        "            probabilities[np.arange(batch_size), true_labels] -= 1\n",
        "\n",
        "            # Gradient of the loss with respect to logits (average over the batch)\n",
        "            gradient = probabilities / batch_size\n",
        "            return gradient\n",
        "\n",
        "        super().__init__(sce, sce_derivative)"
      ],
      "metadata": {
        "id": "_JI57nEWXw5A"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalCrossEntropyLoss(Loss):\n",
        "    def __init__(self):\n",
        "        def cel(y_predSM, y_true):\n",
        "            # Clip predictions for numerical stability to avoid log(0)\n",
        "            y_predSM = np.clip(y_predSM, 1e-9, 1 - 1e-9)\n",
        "            # Compute categorical cross-entropy\n",
        "            return -np.mean(np.sum(y_true * np.log(y_predSM), axis=1))\n",
        "\n",
        "        def cel_derivative(y_predSM, y_true):\n",
        "            # The derivative of categorical cross-entropy with respect to softmax output\n",
        "            return y_predSM - y_true\n",
        "\n",
        "        super().__init__(cel, cel_derivative)"
      ],
      "metadata": {
        "id": "Sh-mOX2rX8Qw"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def train(self, images, labels, layers, hyperpara):\n",
        "        for key, value in hyperpara.items():\n",
        "            setattr(self, key, value)\n",
        "        self.loss_fn = CategoricalCrossEntropyLoss()\n",
        "        self.X = images\n",
        "        self.y = labels\n",
        "        self.layers = layers\n",
        "        self.softmax = Softmax(self.neurons[-1], self.softmax_neurons)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            num_samples = self.X.shape[0]\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, num_samples, self.batch_size):\n",
        "                if (i + self.batch_size == num_samples):\n",
        "                    X_batch = self.X[i:num_samples]\n",
        "                    y_batch = self.y[i:num_samples]\n",
        "                else:\n",
        "                    X_batch = self.X[i:i + self.batch_size]\n",
        "                    y_batch = self.y[i:i + self.batch_size]\n",
        "\n",
        "                ActiPredicted = self.forward(X_batch)\n",
        "                pred_y = self.softmax.forward(ActiPredicted)\n",
        "\n",
        "                loss = self.loss_fn.forward(pred_y, y_batch)\n",
        "                epoch_loss += loss\n",
        "\n",
        "                # Corrected line: Call backward with no additional arguments\n",
        "                loss_gradient = self.loss_fn.backward()\n",
        "\n",
        "                # Backward pass\n",
        "                loss_gradient = self.softmax.backward(loss_gradient, self.learning_rate, self.momentum_coeff)\n",
        "                self.backward(loss_gradient, self.learning_rate, self.momentum_coeff)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}, Average Loss: {epoch_loss / (num_samples / self.batch_size):.4f}\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "\n",
        "    def backward(self, loss_gradient, learning_rate, momentum_coeff):\n",
        "        for layer in reversed(self.layers):\n",
        "            loss_gradient = layer.backward(loss_gradient, learning_rate, momentum_coeff)  # Only pass loss_gradient here\n",
        "\n",
        "    def predict(self, X):\n",
        "        logits = self.forward(X)\n",
        "        probabilities = self.softmax.forward(logits)\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        true_labels = np.argmax(y, axis=1)\n",
        "        accuracy = np.mean(predictions == true_labels)\n",
        "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "0o39msHGYJFF"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters={\n",
        "    \"hiddenLayers\":3,\n",
        "    \"learning_rate\":0.005,\n",
        "    \"batch_size\":32,\n",
        "    \"epochs\":150,\n",
        "    \"ActivationFns\":None,\n",
        "    \"neurons\":None,\n",
        "    \"softmax_neurons\":labels.shape[1],\n",
        "    \"momentum_coeff\":0.9\n",
        "\n",
        "}\n",
        "\n",
        "# change\n",
        "hyperparameters[\"neurons\"]=[32,32,16]\n",
        "hyperparameters[\"ActivationFns\"]=[Relu(),Relu(),Relu()]\n",
        "layers=[]\n",
        "inputsize=images.shape[1]\n",
        "for i in range(hyperparameters[\"hiddenLayers\"]):\n",
        "    outputsize=hyperparameters[\"neurons\"][i]\n",
        "    layers.extend([Dense(inputsize,outputsize),hyperparameters[\"ActivationFns\"][i]])\n",
        "    inputsize=outputsize\n",
        "\n",
        "\n",
        "model=Model()\n",
        "model.train(images,labels,layers,hyperparameters)"
      ],
      "metadata": {
        "id": "MPlBhPQDYYxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7526fa6-dfb7-49fb-a4ed-62a2a99df6c7"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 2.5457\n",
            "Epoch 2, Average Loss: 2.3994\n",
            "Epoch 3, Average Loss: 2.2114\n",
            "Epoch 4, Average Loss: 2.1405\n",
            "Epoch 5, Average Loss: 2.0850\n",
            "Epoch 6, Average Loss: 2.0160\n",
            "Epoch 7, Average Loss: 1.9242\n",
            "Epoch 8, Average Loss: 1.8957\n",
            "Epoch 9, Average Loss: 1.7999\n",
            "Epoch 10, Average Loss: 1.7685\n",
            "Epoch 11, Average Loss: 1.7142\n",
            "Epoch 12, Average Loss: 1.6636\n",
            "Epoch 13, Average Loss: 1.6725\n",
            "Epoch 14, Average Loss: 1.6977\n",
            "Epoch 15, Average Loss: 1.5928\n",
            "Epoch 16, Average Loss: 1.5176\n",
            "Epoch 17, Average Loss: 1.4187\n",
            "Epoch 18, Average Loss: 1.3517\n",
            "Epoch 19, Average Loss: 1.2315\n",
            "Epoch 20, Average Loss: 1.1420\n",
            "Epoch 21, Average Loss: 1.0624\n",
            "Epoch 22, Average Loss: 1.0430\n",
            "Epoch 23, Average Loss: 1.0958\n",
            "Epoch 24, Average Loss: 1.0561\n",
            "Epoch 25, Average Loss: 1.0135\n",
            "Epoch 26, Average Loss: 0.8717\n",
            "Epoch 27, Average Loss: 0.8097\n",
            "Epoch 28, Average Loss: 0.7324\n",
            "Epoch 29, Average Loss: 0.7448\n",
            "Epoch 30, Average Loss: 0.7016\n",
            "Epoch 31, Average Loss: 0.6193\n",
            "Epoch 32, Average Loss: 0.5240\n",
            "Epoch 33, Average Loss: 0.4630\n",
            "Epoch 34, Average Loss: 0.4727\n",
            "Epoch 35, Average Loss: 0.4409\n",
            "Epoch 36, Average Loss: 0.4189\n",
            "Epoch 37, Average Loss: 0.3784\n",
            "Epoch 38, Average Loss: 0.4022\n",
            "Epoch 39, Average Loss: 0.4059\n",
            "Epoch 40, Average Loss: 0.4459\n",
            "Epoch 41, Average Loss: 0.5497\n",
            "Epoch 42, Average Loss: 0.4756\n",
            "Epoch 43, Average Loss: 0.4644\n",
            "Epoch 44, Average Loss: 0.5314\n",
            "Epoch 45, Average Loss: 0.4940\n",
            "Epoch 46, Average Loss: 0.3744\n",
            "Epoch 47, Average Loss: 0.4226\n",
            "Epoch 48, Average Loss: 0.5971\n",
            "Epoch 49, Average Loss: 0.4008\n",
            "Epoch 50, Average Loss: 0.3592\n",
            "Epoch 51, Average Loss: 0.2286\n",
            "Epoch 52, Average Loss: 0.1554\n",
            "Epoch 53, Average Loss: 0.1122\n",
            "Epoch 54, Average Loss: 0.1075\n",
            "Epoch 55, Average Loss: 0.0772\n",
            "Epoch 56, Average Loss: 0.0562\n",
            "Epoch 57, Average Loss: 0.0430\n",
            "Epoch 58, Average Loss: 0.0378\n",
            "Epoch 59, Average Loss: 0.0344\n",
            "Epoch 60, Average Loss: 0.0300\n",
            "Epoch 61, Average Loss: 0.0293\n",
            "Epoch 62, Average Loss: 0.0254\n",
            "Epoch 63, Average Loss: 0.0233\n",
            "Epoch 64, Average Loss: 0.0221\n",
            "Epoch 65, Average Loss: 0.0200\n",
            "Epoch 66, Average Loss: 0.0186\n",
            "Epoch 67, Average Loss: 0.0176\n",
            "Epoch 68, Average Loss: 0.0167\n",
            "Epoch 69, Average Loss: 0.0161\n",
            "Epoch 70, Average Loss: 0.0150\n",
            "Epoch 71, Average Loss: 0.0142\n",
            "Epoch 72, Average Loss: 0.0134\n",
            "Epoch 73, Average Loss: 0.0126\n",
            "Epoch 74, Average Loss: 0.0124\n",
            "Epoch 75, Average Loss: 0.0115\n",
            "Epoch 76, Average Loss: 0.0100\n",
            "Epoch 77, Average Loss: 0.0093\n",
            "Epoch 78, Average Loss: 0.0084\n",
            "Epoch 79, Average Loss: 0.0080\n",
            "Epoch 80, Average Loss: 0.0075\n",
            "Epoch 81, Average Loss: 0.0072\n",
            "Epoch 82, Average Loss: 0.0069\n",
            "Epoch 83, Average Loss: 0.0066\n",
            "Epoch 84, Average Loss: 0.0063\n",
            "Epoch 85, Average Loss: 0.0061\n",
            "Epoch 86, Average Loss: 0.0058\n",
            "Epoch 87, Average Loss: 0.0056\n",
            "Epoch 88, Average Loss: 0.0054\n",
            "Epoch 89, Average Loss: 0.0052\n",
            "Epoch 90, Average Loss: 0.0050\n",
            "Epoch 91, Average Loss: 0.0048\n",
            "Epoch 92, Average Loss: 0.0048\n",
            "Epoch 93, Average Loss: 0.0045\n",
            "Epoch 94, Average Loss: 0.0044\n",
            "Epoch 95, Average Loss: 0.0042\n",
            "Epoch 96, Average Loss: 0.0041\n",
            "Epoch 97, Average Loss: 0.0040\n",
            "Epoch 98, Average Loss: 0.0039\n",
            "Epoch 99, Average Loss: 0.0038\n",
            "Epoch 100, Average Loss: 0.0037\n",
            "Epoch 101, Average Loss: 0.0036\n",
            "Epoch 102, Average Loss: 0.0035\n",
            "Epoch 103, Average Loss: 0.0034\n",
            "Epoch 104, Average Loss: 0.0033\n",
            "Epoch 105, Average Loss: 0.0032\n",
            "Epoch 106, Average Loss: 0.0032\n",
            "Epoch 107, Average Loss: 0.0031\n",
            "Epoch 108, Average Loss: 0.0030\n",
            "Epoch 109, Average Loss: 0.0030\n",
            "Epoch 110, Average Loss: 0.0029\n",
            "Epoch 111, Average Loss: 0.0028\n",
            "Epoch 112, Average Loss: 0.0028\n",
            "Epoch 113, Average Loss: 0.0027\n",
            "Epoch 114, Average Loss: 0.0027\n",
            "Epoch 115, Average Loss: 0.0026\n",
            "Epoch 116, Average Loss: 0.0025\n",
            "Epoch 117, Average Loss: 0.0025\n",
            "Epoch 118, Average Loss: 0.0024\n",
            "Epoch 119, Average Loss: 0.0024\n",
            "Epoch 120, Average Loss: 0.0023\n",
            "Epoch 121, Average Loss: 0.0023\n",
            "Epoch 122, Average Loss: 0.0022\n",
            "Epoch 123, Average Loss: 0.0022\n",
            "Epoch 124, Average Loss: 0.0021\n",
            "Epoch 125, Average Loss: 0.0021\n",
            "Epoch 126, Average Loss: 0.0021\n",
            "Epoch 127, Average Loss: 0.0020\n",
            "Epoch 128, Average Loss: 0.0020\n",
            "Epoch 129, Average Loss: 0.0019\n",
            "Epoch 130, Average Loss: 0.0019\n",
            "Epoch 131, Average Loss: 0.0019\n",
            "Epoch 132, Average Loss: 0.0019\n",
            "Epoch 133, Average Loss: 0.0018\n",
            "Epoch 134, Average Loss: 0.0018\n",
            "Epoch 135, Average Loss: 0.0018\n",
            "Epoch 136, Average Loss: 0.0017\n",
            "Epoch 137, Average Loss: 0.0017\n",
            "Epoch 138, Average Loss: 0.0017\n",
            "Epoch 139, Average Loss: 0.0017\n",
            "Epoch 140, Average Loss: 0.0016\n",
            "Epoch 141, Average Loss: 0.0016\n",
            "Epoch 142, Average Loss: 0.0016\n",
            "Epoch 143, Average Loss: 0.0016\n",
            "Epoch 144, Average Loss: 0.0016\n",
            "Epoch 145, Average Loss: 0.0015\n",
            "Epoch 146, Average Loss: 0.0015\n",
            "Epoch 147, Average Loss: 0.0015\n",
            "Epoch 148, Average Loss: 0.0015\n",
            "Epoch 149, Average Loss: 0.0015\n",
            "Epoch 150, Average Loss: 0.0014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"accuary:{model.evaluate(test_images,test_labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcv8DU5UYpyM",
        "outputId": "d1a02eae-8347-4e73-9aae-17c9c383bb8d"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 73.39%\n",
            "accuary:0.7339055793991416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDBszGxsbYVE"
      },
      "execution_count": 135,
      "outputs": []
    }
  ]
}